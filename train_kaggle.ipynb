{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335e636c",
   "metadata": {},
   "source": [
    "# Dental Disease Classification - Training on Kaggle\n",
    "\n",
    "Notebook n√†y ƒë·ªÉ train CNN model ph√°t hi·ªán b·ªánh rƒÉng mi·ªáng tr·ª±c ti·∫øp tr√™n Kaggle.\n",
    "\n",
    "**Dataset**: [Oral Diseases by salmansajid05](https://www.kaggle.com/datasets/salmansajid05/oral-diseases)\n",
    "\n",
    "**Classes**:\n",
    "- Normal (rƒÉng kh·ªèe m·∫°nh)\n",
    "- Cavity (s√¢u rƒÉng)\n",
    "- Gingivitis (vi√™m n∆∞·ªõu)\n",
    "- Plaque (m·∫£ng b√°m)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è L∆ØU √ù QUAN TR·ªåNG:\n",
    "**PH·∫¢I ch·∫°y c√°c cell theo th·ª© t·ª± t·ª´ tr√™n xu·ªëng d∆∞·ªõi!**\n",
    "1. Kh√¥ng skip b·∫•t k·ª≥ cell n√†o\n",
    "2. ƒê·ª£i cell tr∆∞·ªõc ch·∫°y xong m·ªõi ch·∫°y cell ti·∫øp theo\n",
    "3. ƒê·∫∑c bi·ªát cell \"Train Model\" ph·∫£i ch·∫°y xong tr∆∞·ªõc khi ch·∫°y visualization\n",
    "\n",
    "**Recommended:** Click \"Run All\" ƒë·ªÉ ch·∫°y t·∫•t c·∫£ cells theo th·ª© t·ª± t·ª± ƒë·ªông."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "print(\"‚úÖ Single GPU mode (most stable)\")\n",
    "print(\"‚úÖ This avoids Multi-GPU synchronization issues\")\n",
    "\n",
    "# Check GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"‚úÖ Available GPUs: {len(gpus)}\")\n",
    "\n",
    "# Set memory growth to prevent OOM\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è {e}\")\n",
    "\n",
    "print(f\"\\nüöÄ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üöÄ Training config: Single GPU + float32 (stable training)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f88c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path tr√™n Kaggle\n",
    "dataset_path = '/kaggle/input/oral-diseases'\n",
    "\n",
    "# Ki·ªÉm tra c·∫•u tr√∫c dataset\n",
    "print(\"üìÅ Dataset structure:\")\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    level = root.replace(dataset_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    if level < 2:  # Ch·ªâ show 2 levels\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:3]:  # Show 3 files ƒë·∫ßu\n",
    "            print(f'{subindent}{file}')\n",
    "        if len(files) > 3:\n",
    "            print(f'{subindent}... and {len(files)-3} more files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e25538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32  # Single GPU - kh√¥ng nh√¢n v·ªõi num_replicas\n",
    "EPOCHS = 30\n",
    "\n",
    "print(f\"üìä Batch size: {BATCH_SIZE}\")\n",
    "print(f\"üìä Single GPU training\")\n",
    "\n",
    "# Data augmentation - lightweight\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Try structure with train/val folders first\n",
    "try:\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    val_dir = os.path.join(dataset_path, 'val')\n",
    "    \n",
    "    if os.path.exists(train_dir) and os.path.exists(val_dir):\n",
    "        print(\"‚úÖ Detected train/val folder structure\")\n",
    "        \n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            val_dir,\n",
    "            target_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False\n",
    "        )\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Train/val folders not found\")\n",
    "        \n",
    "except:\n",
    "    # Fallback: single folder with validation_split\n",
    "    print(\"‚úÖ Using single folder structure with validation_split=0.2\")\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = train_datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "print(f\"\\nüìä Training samples: {train_generator.samples}\")\n",
    "print(f\"üìä Validation samples: {val_generator.samples}\")\n",
    "print(f\"üìÇ Classes: {train_generator.class_indices}\")\n",
    "print(f\"üíæ Memory optimization: Batch size 32, no caching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bda20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Build model - NO strategy scope (single GPU)\n",
    "model = keras.Sequential([\n",
    "    # Block 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 3\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 4\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Classifier\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Optimizer v·ªõi learning rate conservative\n",
    "optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    clipvalue=1.0  # Gradient clipping\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model built (single GPU, no strategy)\")\n",
    "print(\"‚úÖ Gradient clipping enabled\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479acc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_dental_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# NaN check callback\n",
    "class NaNCheckCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs and (np.isnan(logs['loss']) or np.isnan(logs['accuracy'])):\n",
    "            print(\"\\n‚ùå NaN detected! Stopping training...\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "nan_check = NaNCheckCallback()\n",
    "\n",
    "print(f\"üöÄ Training Configuration:\")\n",
    "print(f\"  ‚îú‚îÄ GPU: Single T4 GPU (no Multi-GPU overhead)\")\n",
    "print(f\"  ‚îú‚îÄ Precision: float32 (stable)\")\n",
    "print(f\"  ‚îú‚îÄ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  ‚îú‚îÄ Learning rate: 0.001\")\n",
    "print(f\"  ‚îú‚îÄ Gradient clipping: 1.0\")\n",
    "print(f\"  ‚îú‚îÄ Model: Simplified (no BatchNorm)\")\n",
    "print(f\"  ‚îî‚îÄ Total epochs: {EPOCHS}\")\n",
    "print(\"\\n‚è≥ Starting training...\")\n",
    "print(\"üí° Single GPU = simpler, more stable, still fast!\")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = val_generator.samples // BATCH_SIZE\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr, nan_check],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training was completed\n",
    "try:\n",
    "    history.history\n",
    "except NameError:\n",
    "    print(\"‚ùå Error: Training ch∆∞a ƒë∆∞·ª£c ch·∫°y!\")\n",
    "    print(\"üëâ H√£y ch·∫°y cell 'B∆∞·ªõc 5: Train Model' tr∆∞·ªõc khi ch·∫°y cell n√†y\")\n",
    "    raise NameError(\"Bi·∫øn 'history' ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a. Ch·∫°y cell training tr∆∞·ªõc!\")\n",
    "\n",
    "# Plot accuracy and loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\nüìà Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"üìà Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"üìâ Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"üìâ Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model and training data exist\n",
    "try:\n",
    "    model\n",
    "    val_generator\n",
    "except NameError:\n",
    "    print(\"‚ùå Error: Model ho·∫∑c validation data ch∆∞a c√≥!\")\n",
    "    print(\"üëâ Ch·∫°y c√°c cell tr∆∞·ªõc ƒë√≥ theo th·ª© t·ª± t·ª´ B∆∞·ªõc 1 ƒë·∫øn B∆∞·ªõc 5\")\n",
    "    raise NameError(\"H√£y ch·∫°y c√°c cell training tr∆∞·ªõc!\")\n",
    "\n",
    "# Get predictions\n",
    "val_generator.reset()\n",
    "predictions = model.predict(val_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = val_generator.classes\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=train_generator.class_indices.keys(),\n",
    "            yticklabels=train_generator.class_indices.keys())\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, \n",
    "                          target_names=train_generator.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665983f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model exists\n",
    "try:\n",
    "    model\n",
    "    history\n",
    "    train_generator\n",
    "except NameError:\n",
    "    print(\"‚ùå Error: Model ch∆∞a ƒë∆∞·ª£c train!\")\n",
    "    print(\"üëâ Ch·∫°y t·∫•t c·∫£ c√°c cell t·ª´ ƒë·∫ßu theo th·ª© t·ª±\")\n",
    "    raise NameError(\"H√£y train model tr∆∞·ªõc khi save!\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = '/kaggle/working/dental_model_final.h5'\n",
    "model.save(final_model_path)\n",
    "print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
    "\n",
    "# Also save as backup\n",
    "model.save('/kaggle/working/dental_model_backup.h5')\n",
    "print(\"‚úÖ Backup model saved!\")\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "history_path = '/kaggle/working/training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "print(f\"‚úÖ Training history saved to: {history_path}\")\n",
    "\n",
    "# Save class indices\n",
    "class_indices_path = '/kaggle/working/class_indices.json'\n",
    "with open(class_indices_path, 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f)\n",
    "print(f\"‚úÖ Class indices saved to: {class_indices_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING HO√ÄN T·∫§T!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüì• Download c√°c file n√†y t·ª´ Output c·ªßa notebook:\")\n",
    "print(\"  1. dental_model_final.h5 - Model ch√≠nh\")\n",
    "print(\"  2. dental_model_backup.h5 - Model backup\")\n",
    "print(\"  3. training_history.json - L·ªãch s·ª≠ training\")\n",
    "print(\"  4. class_indices.json - Mapping c√°c class\")\n",
    "print(\"\\nüí° B·ªè file dental_model_final.h5 v√†o th∆∞ m·ª•c models/ c·ªßa project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562be8c5",
   "metadata": {},
   "source": [
    "## üìù H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng tr√™n Kaggle\n",
    "\n",
    "### 1. T·∫°o Notebook m·ªõi tr√™n Kaggle\n",
    "- V√†o https://www.kaggle.com/\n",
    "- Click \"Create\" ‚Üí \"New Notebook\"\n",
    "\n",
    "### 2. Add Dataset\n",
    "- Click \"Add Data\" ·ªü panel b√™n ph·∫£i\n",
    "- T√¨m \"oral-diseases\" by salmansajid05\n",
    "- Click \"Add\" ƒë·ªÉ th√™m v√†o notebook\n",
    "\n",
    "### 3. Enable GPU (**B·∫ÆT BU·ªòC** - Ch·ªçn 2 GPUs!)\n",
    "- Click \"Session Options\" (bi·ªÉu t∆∞·ª£ng b√°nh rƒÉng)\n",
    "- Ch·ªçn \"Accelerator\" ‚Üí **\"GPU T4 x2\"** (2 GPUs ƒë·ªÉ tƒÉng t·ªëc g·∫•p ƒë√¥i!)\n",
    "- Click \"Save\"\n",
    "\n",
    "### 4. Run All Cells\n",
    "- Click \"Run All\" ho·∫∑c ch·∫°y t·ª´ng cell\n",
    "- ‚ö° **Ch·ªâ m·∫•t kho·∫£ng 5-10 ph√∫t** v·ªõi 2 T4 GPUs (thay v√¨ 30-60 ph√∫t!)\n",
    "\n",
    "### 5. Download Model\n",
    "- Sau khi ch·∫°y xong, v√†o tab \"Output\" ·ªü panel b√™n ph·∫£i\n",
    "- Download file `dental_model_final.h5`\n",
    "- Copy v√†o th∆∞ m·ª•c `models/` c·ªßa project local\n",
    "\n",
    "### 6. Test Model Local\n",
    "```python\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('models/dental_model_final.h5')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ T·ªëi ∆∞u h√≥a ƒë√£ √°p d·ª•ng:\n",
    "\n",
    "1. **Multi-GPU Training** (2x T4):\n",
    "   - T·ª± ƒë·ªông ph√¢n ph·ªëi training tr√™n 2 GPUs\n",
    "   - TƒÉng t·ªëc **2x** t·ª´ parallel processing\n",
    "   \n",
    "2. **Mixed Precision (float16)**:\n",
    "   - Gi·∫£m memory usage 50%\n",
    "   - TƒÉng t·ªëc **2-3x** tr√™n Tensor Cores c·ªßa T4\n",
    "   \n",
    "3. **XLA (Accelerated Linear Algebra)**:\n",
    "   - Just-in-time compilation\n",
    "   - TƒÉng t·ªëc th√™m **10-30%**\n",
    "   \n",
    "4. **Batch Size Scaling**:\n",
    "   - 64 per GPU = 128 total batch size\n",
    "   - Faster convergence v·ªõi large batches\n",
    "   \n",
    "5. **Data Pipeline Optimization**:\n",
    "   - `cache()`: Cache data in memory\n",
    "   - `prefetch()`: Load next batch trong khi training\n",
    "   - Lo·∫°i b·ªè I/O bottleneck\n",
    "   \n",
    "6. **Learning Rate Scaling**:\n",
    "   - T·ª± ƒë·ªông scale theo s·ªë GPUs\n",
    "   - Stable training v·ªõi large batches\n",
    "\n",
    "**K·∫øt qu·∫£:** Training nhanh h∆°n **4-6 l·∫ßn** so v·ªõi single GPU kh√¥ng optimize! ‚ö°\n",
    "\n",
    "---\n",
    "\n",
    "**L∆∞u √Ω:**\n",
    "- Dataset t·ª± ƒë·ªông c√≥ t·∫°i `/kaggle/input/oral-diseases/`\n",
    "- Model t·ª± ƒë·ªông l∆∞u v√†o `/kaggle/working/`\n",
    "- **PH·∫¢I ch·ªçn GPU T4 x2** ƒë·ªÉ c√≥ 2 GPUs\n",
    "- Training time: ~5-10 ph√∫t thay v√¨ 30-60 ph√∫t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29d5a2",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 8: Save Model\n",
    "\n",
    "Model s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o `/kaggle/working/` v√† t·ª± ƒë·ªông c√≥ trong Output c·ªßa notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5e4c5",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 7: Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e195d63",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 6: Visualize Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cadd1",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807235d7",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4: Build CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de677f",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: Auto-detect Dataset Structure & Create Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e388e3",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: Setup Dataset Path\n",
    "\n",
    "**L∆∞u √Ω**: Tr√™n Kaggle, add dataset v√†o notebook:\n",
    "1. Click \"Add Data\" ·ªü b√™n ph·∫£i\n",
    "2. T√¨m \"oral-diseases\" by salmansajid05\n",
    "3. Add v√†o notebook\n",
    "4. Dataset s·∫Ω c√≥ t·∫°i `/kaggle/input/oral-diseases/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e27877",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: Import Libraries"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
